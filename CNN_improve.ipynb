{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/rifkiaputri/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/rifkiaputri/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english')).union(set(ENGLISH_STOP_WORDS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import csv\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "\n",
    "class Mydataset(Dataset):\n",
    "    def __init__(self, train=True):\n",
    "        def clean_str(string):\n",
    "            return \" \".join(re.findall(r'\\w+', string, flags=re.UNICODE)).lower()\n",
    "        if train:\n",
    "            print('loading training dataset')\n",
    "            bf = 'train_bodies.csv'\n",
    "            sf = 'train_stances.csv'\n",
    "        else:\n",
    "            print('loading testing dataset')\n",
    "            bf = 'competition_test_bodies.csv'\n",
    "            sf = 'competition_test_stances.csv'\n",
    "            \n",
    "        with open(os.path.join('fnc-1', sf), 'r', newline='', encoding='utf-8') as myFile:  \n",
    "            rdr = csv.reader(myFile)\n",
    "            next(rdr)\n",
    "            temp = list(rdr)\n",
    "            self.stances = [[clean_str(a[0]), a[1], a[2]] for a in temp]\n",
    "            print(len(self.stances), 'stances')\n",
    "\n",
    "        with open(os.path.join('fnc-1', bf), 'r', newline='', encoding='utf-8') as myFile:  \n",
    "            rdr = csv.reader(myFile)\n",
    "            next(rdr)\n",
    "            temp = list(rdr)\n",
    "            self.bodies = dict([[a[0], clean_str(a[1])]for a in temp])\n",
    "            print(len(self.bodies), 'bodies')\n",
    "\n",
    "        self.len = len(self.stances)\n",
    "        self.labels = list(sorted(set([t[2] for t in self.stances])))\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.stances[index][0], self.bodies[self.stances[index][1]], self.stances[index][2]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "        \n",
    "    def get_labels(self):\n",
    "        return self.labels\n",
    "    \n",
    "    def get_label(self, id):\n",
    "        return self.labels[id]\n",
    "    \n",
    "    def get_label_id(self, label):\n",
    "        return self.labels.index(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading training dataset\n",
      "49972 stances\n",
      "1683 bodies\n",
      "loading testing dataset\n",
      "25413 stances\n",
      "904 bodies\n"
     ]
    }
   ],
   "source": [
    "train_dataset = Mydataset()\n",
    "test_dataset = Mydataset(train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "test_loader = DataLoader(dataset=test_dataset,\n",
    "                         batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "N_LABELS = len(train_dataset.get_labels())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read vector file...\n",
      "Initialize word vector array...\n",
      "Convert word vector to tensor...\n"
     ]
    }
   ],
   "source": [
    "import word2vec.wordvector as w2v\n",
    "\n",
    "class args:\n",
    "    pass\n",
    "\n",
    "args = args()\n",
    "\n",
    "# Assign embedding param in different cell because it takes some time to load\n",
    "args.embed, args.embeding_num, args.embeding_dim = w2v.get_embedding(vec_file='./word2vec/data/vec_50.txt', dim=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.class_num = N_LABELS\n",
    "args.kernel_num = 16\n",
    "args.kernel_sizes = [3,4,5]\n",
    "args.dropout = 0.5\n",
    "args.static = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dic = {}\n",
    "with open(os.path.join('word2vec', 'data', 'word_to_id.txt'), 'r', encoding='utf-8') as f:\n",
    "    i = 1\n",
    "    for line in f:\n",
    "        word = line.strip()\n",
    "        word_dic[word] = i\n",
    "        i += 1\n",
    "\n",
    "def get_id(word):\n",
    "    return word_dic.get(word, 0)\n",
    "\n",
    "_wnl = nltk.WordNetLemmatizer()\n",
    "\n",
    "def normalize_word(w):\n",
    "    return _wnl.lemmatize(w).lower()\n",
    "\n",
    "def get_tokenized_lemmas(s):\n",
    "    return [normalize_word(t) for t in nltk.word_tokenize(s)]\n",
    "\n",
    "def build_tensor(titles, bodys, labels):\n",
    "    label_t = torch.tensor([train_dataset.get_label_id(l) for l in labels], dtype=torch.long, device=device)\n",
    "    \n",
    "    title_t = [torch.tensor([get_id(w) for w in get_tokenized_lemmas(title) if w not in stop_words], dtype=torch.long, device=device) for title in titles]\n",
    "    title_l = [a.shape[0] for a in title_t]\n",
    "    title_max = max(title_l)\n",
    "    title_p = [title_max - a for a in title_l]\n",
    "    title_t = [F.pad(a.view(1,1,1,-1), (0, title_p[i], 0, 0)).view(1,-1) for i, a in enumerate(title_t)]\n",
    "    \n",
    "    body_t = [torch.tensor([get_id(w) for w in get_tokenized_lemmas(body) if w not in stop_words], dtype=torch.long, device=device) for body in bodys]\n",
    "    body_l = [a.shape[0] for a in body_t]\n",
    "    body_max = max(body_l)\n",
    "    body_p = [body_max - a for a in body_l]\n",
    "    body_t = [F.pad(a.view(1,1,1,-1), (0, body_p[i], 0, 0)).view(1,-1) for i, a in enumerate(body_t)]\n",
    "    return torch.cat(title_t, 0), torch.cat(body_t, 0), label_t "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(test_loader, model, args):\n",
    "    model.eval()\n",
    "    corrects, avg_loss, total = 0, 0, 0\n",
    "    for title, body, label in test_loader:\n",
    "        title_t, body_t, label_t = build_tensor(title, body, label)\n",
    "        logit = model(title_t, body_t)\n",
    "        loss = F.cross_entropy(logit, label_t, size_average=False)\n",
    "        avg_loss += loss.data[0]\n",
    "        corrects += (torch.max(logit, 1)[1].view(label_t.size()).data == label_t.data).sum()\n",
    "        total += len(title)\n",
    "\n",
    "    size = len(test_loader.dataset)\n",
    "    avg_loss /= size\n",
    "    accuracy = 100.0 * corrects/size\n",
    "    print('Evaluation - loss: {:.6f}  acc: {:.4f}%({}/{})'.format(avg_loss, \n",
    "                                                                       accuracy, \n",
    "                                                                       corrects, \n",
    "                                                                       size))\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(model, save_dir, save_prefix, steps):\n",
    "    if not os.path.isdir(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    save_prefix = os.path.join(save_dir, save_prefix)\n",
    "    save_path = '{}_steps_{}.pt'.format(save_prefix, steps)\n",
    "    torch.save(model.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN_Text(\n",
       "  (embed): Embedding(3000001, 50)\n",
       "  (convs1): ModuleList(\n",
       "    (0): Conv2d(1, 16, kernel_size=(3, 50), stride=(1, 1))\n",
       "    (1): Conv2d(1, 16, kernel_size=(4, 50), stride=(1, 1))\n",
       "    (2): Conv2d(1, 16, kernel_size=(5, 50), stride=(1, 1))\n",
       "  )\n",
       "  (convs2): ModuleList(\n",
       "    (0): Conv2d(1, 16, kernel_size=(3, 50), stride=(1, 1))\n",
       "    (1): Conv2d(1, 16, kernel_size=(4, 50), stride=(1, 1))\n",
       "    (2): Conv2d(1, 16, kernel_size=(5, 50), stride=(1, 1))\n",
       "  )\n",
       "  (dropout): Dropout(p=0.5)\n",
       "  (fc1): Linear(in_features=48, out_features=48, bias=True)\n",
       "  (fc2): Linear(in_features=48, out_features=48, bias=True)\n",
       "  (fc_final): Linear(in_features=48, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class CNN_Text(nn.Module):\n",
    "    \n",
    "    def __init__(self, args):\n",
    "        super(CNN_Text, self).__init__()\n",
    "        self.args = args\n",
    "        \n",
    "        C = args.class_num\n",
    "        Ci = 1\n",
    "        Co = args.kernel_num\n",
    "        Ks = args.kernel_sizes\n",
    "        V = args.embeding_num\n",
    "        D = args.embeding_dim\n",
    "        self.embed = args.embed\n",
    "\n",
    "        self.convs1 = nn.ModuleList([nn.Conv2d(Ci, Co, (K, D)) for K in Ks])\n",
    "        \n",
    "        self.convs2 = nn.ModuleList([nn.Conv2d(Ci, Co, (K, D)) for K in Ks])\n",
    "\n",
    "        self.dropout = nn.Dropout(args.dropout)\n",
    "\n",
    "        self.fc1 = nn.Linear(len(Ks) * Co, len(Ks) * Co)\n",
    "        \n",
    "        self.fc2 = nn.Linear(len(Ks) * Co, len(Ks) * Co)\n",
    "        \n",
    "        self.fc_final = nn.Linear(len(Ks) * Co, C)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        # Title\n",
    "        x = self.embed(x)  # (N, W, D)\n",
    "\n",
    "        if self.args.static:\n",
    "            x = Variable(x)\n",
    "\n",
    "        x = x.unsqueeze(1)  # (N, Ci, W, D)\n",
    "        x = [F.relu(conv(x)).squeeze(3) for conv in self.convs1]  # [(N, Co, W), ...]*len(Ks)\n",
    "        x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x]  # [(N, Co), ...]*len(Ks)\n",
    "        x = torch.cat(x, 1)\n",
    "        \n",
    "        # Body text\n",
    "        y = self.embed(y)  # (N, W, D)\n",
    "\n",
    "        if self.args.static:\n",
    "            y = Variable(y)\n",
    "\n",
    "        y = y.unsqueeze(1)  # (N, Ci, W, D)\n",
    "        y = [F.relu(conv(y)).squeeze(3) for conv in self.convs2]  # [(N, Co, W), ...]*len(Ks)\n",
    "        y = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in y]  # [(N, Co), ...]*len(Ks)\n",
    "        y = torch.cat(y, 1)\n",
    "        \n",
    "        # Calculate distance between x and y\n",
    "        out = torch.mul(x, y)\n",
    "        \n",
    "        # Fully connected nn\n",
    "        out = self.fc1(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.dropout(out)  # (N, len(Ks)*Co)\n",
    "        out = self.fc_final(out)  # (N, C)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    \n",
    "model = CNN_Text(args)\n",
    "model.double()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.lr = 0.001\n",
    "args.epochs = 10\n",
    "args.log_interval = 10\n",
    "args.test_interval = 400\n",
    "args.save_interval = 1000\n",
    "args.save_dir = 'models'  # model save path\n",
    "if not os.path.exists(args.save_dir):\n",
    "    os.mkdir(args.save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start epoch 1 ....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rifkiaputri/anaconda2/envs/nlp_project/lib/python3.6/site-packages/ipykernel_launcher.py:23: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch[10] - loss: 1.234055  acc: 70.0000%(90/128)\n",
      "Batch[20] - loss: 1.111588  acc: 67.0000%(86/128)\n",
      "Batch[30] - loss: 0.850963  acc: 74.0000%(95/128)\n",
      "Batch[40] - loss: 0.727852  acc: 78.0000%(101/128)\n",
      "Batch[50] - loss: 0.874618  acc: 72.0000%(93/128)\n",
      "Batch[60] - loss: 0.781356  acc: 73.0000%(94/128)\n",
      "Batch[70] - loss: 0.790848  acc: 73.0000%(94/128)\n",
      "Batch[80] - loss: 0.756115  acc: 76.0000%(98/128)\n",
      "Batch[90] - loss: 0.847405  acc: 72.0000%(93/128)\n",
      "Batch[100] - loss: 0.766590  acc: 73.0000%(94/128)\n",
      "Batch[110] - loss: 0.858218  acc: 68.0000%(88/128)\n",
      "Batch[120] - loss: 0.792750  acc: 75.0000%(96/128)\n",
      "Batch[130] - loss: 0.850287  acc: 72.0000%(93/128)\n",
      "Batch[140] - loss: 0.899479  acc: 71.0000%(91/128)\n",
      "Batch[150] - loss: 0.870599  acc: 67.0000%(86/128)\n",
      "Batch[160] - loss: 0.893462  acc: 65.0000%(84/128)\n",
      "Batch[170] - loss: 0.713164  acc: 76.0000%(98/128)\n",
      "Batch[180] - loss: 0.731306  acc: 75.0000%(97/128)\n",
      "Batch[190] - loss: 0.985043  acc: 67.0000%(86/128)\n",
      "Batch[200] - loss: 0.736531  acc: 76.0000%(98/128)\n",
      "Batch[210] - loss: 0.882139  acc: 67.0000%(86/128)\n",
      "Batch[220] - loss: 0.738870  acc: 74.0000%(95/128)\n",
      "Batch[230] - loss: 0.844426  acc: 68.0000%(88/128)\n",
      "Batch[240] - loss: 0.748421  acc: 71.0000%(91/128)\n",
      "Batch[250] - loss: 0.808691  acc: 73.0000%(94/128)\n",
      "Batch[260] - loss: 0.918650  acc: 70.0000%(90/128)\n",
      "Batch[270] - loss: 0.795474  acc: 73.0000%(94/128)\n",
      "Batch[280] - loss: 0.852865  acc: 69.0000%(89/128)\n",
      "Batch[290] - loss: 0.839745  acc: 68.0000%(88/128)\n",
      "Batch[300] - loss: 0.713367  acc: 78.0000%(100/128)\n",
      "Batch[310] - loss: 0.701975  acc: 75.0000%(97/128)\n",
      "Batch[320] - loss: 0.668072  acc: 79.0000%(102/128)\n",
      "Batch[330] - loss: 0.759925  acc: 72.0000%(93/128)\n",
      "Batch[340] - loss: 0.655294  acc: 79.0000%(102/128)\n",
      "Batch[350] - loss: 0.781121  acc: 71.0000%(91/128)\n",
      "Batch[360] - loss: 0.769573  acc: 70.0000%(90/128)\n",
      "Batch[370] - loss: 0.636242  acc: 78.0000%(100/128)\n",
      "Batch[380] - loss: 0.810848  acc: 68.0000%(88/128)\n",
      "Batch[390] - loss: 0.790727  acc: 69.0000%(89/128)\n",
      "\n",
      "Start epoch 2 ....\n",
      "Batch[400] - loss: 0.651997  acc: 78.0000%(100/128)\n",
      "Batch[410] - loss: 0.665112  acc: 75.0000%(97/128)\n",
      "Batch[420] - loss: 0.656017  acc: 78.0000%(100/128)\n",
      "Batch[430] - loss: 0.727487  acc: 75.0000%(96/128)\n",
      "Batch[440] - loss: 0.642391  acc: 74.0000%(95/128)\n",
      "Batch[450] - loss: 0.700616  acc: 76.0000%(98/128)\n",
      "Batch[460] - loss: 0.629702  acc: 76.0000%(98/128)\n",
      "Batch[470] - loss: 0.559680  acc: 81.0000%(104/128)\n",
      "Batch[480] - loss: 0.693070  acc: 75.0000%(97/128)\n",
      "Batch[490] - loss: 0.663811  acc: 71.0000%(92/128)\n",
      "Batch[500] - loss: 0.515272  acc: 84.0000%(108/128)\n",
      "Batch[510] - loss: 0.710113  acc: 74.0000%(95/128)\n",
      "Batch[520] - loss: 0.762580  acc: 75.0000%(96/128)\n",
      "Batch[530] - loss: 0.651620  acc: 75.0000%(96/128)\n",
      "Batch[540] - loss: 0.561329  acc: 77.0000%(99/128)\n",
      "Batch[550] - loss: 0.650054  acc: 73.0000%(94/128)\n",
      "Batch[560] - loss: 0.634125  acc: 78.0000%(100/128)\n",
      "Batch[570] - loss: 0.718761  acc: 72.0000%(93/128)\n",
      "Batch[580] - loss: 0.861109  acc: 67.0000%(86/128)\n",
      "Batch[590] - loss: 0.512461  acc: 82.0000%(105/128)\n",
      "Batch[600] - loss: 0.682543  acc: 76.0000%(98/128)\n",
      "Batch[610] - loss: 0.401165  acc: 87.0000%(112/128)\n",
      "Batch[620] - loss: 0.693426  acc: 75.0000%(96/128)\n",
      "Batch[630] - loss: 0.429087  acc: 86.0000%(111/128)\n",
      "Batch[640] - loss: 0.446084  acc: 83.0000%(107/128)\n",
      "Batch[650] - loss: 0.592468  acc: 80.0000%(103/128)\n",
      "Batch[660] - loss: 0.417805  acc: 89.0000%(114/128)\n",
      "Batch[670] - loss: 0.492094  acc: 78.0000%(101/128)\n",
      "Batch[680] - loss: 0.592964  acc: 82.0000%(106/128)\n",
      "Batch[690] - loss: 0.393853  acc: 84.0000%(108/128)\n",
      "Batch[700] - loss: 0.552359  acc: 80.0000%(103/128)\n",
      "Batch[710] - loss: 0.494827  acc: 83.0000%(107/128)\n",
      "Batch[720] - loss: 0.558173  acc: 75.0000%(97/128)\n",
      "Batch[730] - loss: 0.715086  acc: 73.0000%(94/128)\n",
      "Batch[740] - loss: 0.391272  acc: 87.0000%(112/128)\n",
      "Batch[750] - loss: 0.459586  acc: 83.0000%(107/128)\n",
      "Batch[760] - loss: 0.477314  acc: 80.0000%(103/128)\n",
      "Batch[770] - loss: 0.600055  acc: 78.0000%(100/128)\n",
      "Batch[780] - loss: 0.515113  acc: 82.0000%(105/128)\n",
      "\n",
      "Start epoch 3 ....\n",
      "Batch[790] - loss: 0.487625  acc: 82.0000%(105/128)\n",
      "Batch[800] - loss: 0.500273  acc: 82.0000%(106/128)\n",
      "Batch[810] - loss: 0.491318  acc: 85.0000%(110/128)\n",
      "Batch[820] - loss: 0.501837  acc: 82.0000%(105/128)\n",
      "Batch[830] - loss: 0.569292  acc: 78.0000%(101/128)\n",
      "Batch[840] - loss: 0.478952  acc: 84.0000%(108/128)\n",
      "Batch[850] - loss: 0.532092  acc: 81.0000%(104/128)\n",
      "Batch[860] - loss: 0.415471  acc: 84.0000%(108/128)\n",
      "Batch[870] - loss: 0.371649  acc: 85.0000%(110/128)\n",
      "Batch[880] - loss: 0.400100  acc: 88.0000%(113/128)\n",
      "Batch[890] - loss: 0.501531  acc: 80.0000%(103/128)\n",
      "Batch[900] - loss: 0.278202  acc: 87.0000%(112/128)\n",
      "Batch[910] - loss: 0.549223  acc: 82.0000%(105/128)\n",
      "Batch[920] - loss: 0.555218  acc: 79.0000%(102/128)\n",
      "Batch[930] - loss: 0.333456  acc: 91.0000%(117/128)\n",
      "Batch[940] - loss: 0.304469  acc: 90.0000%(116/128)\n",
      "Batch[950] - loss: 0.411431  acc: 87.0000%(112/128)\n",
      "Batch[960] - loss: 0.383545  acc: 86.0000%(111/128)\n",
      "Batch[970] - loss: 0.554570  acc: 78.0000%(100/128)\n",
      "Batch[980] - loss: 0.473804  acc: 81.0000%(104/128)\n",
      "Batch[990] - loss: 0.430972  acc: 84.0000%(108/128)\n",
      "Batch[1000] - loss: 0.452998  acc: 82.0000%(106/128)\n",
      "Batch[1010] - loss: 0.445046  acc: 82.0000%(106/128)\n",
      "Batch[1020] - loss: 0.466190  acc: 81.0000%(104/128)\n",
      "Batch[1030] - loss: 0.374816  acc: 88.0000%(113/128)\n",
      "Batch[1040] - loss: 0.400459  acc: 86.0000%(111/128)\n",
      "Batch[1050] - loss: 0.452681  acc: 83.0000%(107/128)\n",
      "Batch[1060] - loss: 0.551072  acc: 83.0000%(107/128)\n",
      "Batch[1070] - loss: 0.459029  acc: 84.0000%(108/128)\n",
      "Batch[1080] - loss: 0.399692  acc: 82.0000%(105/128)\n",
      "Batch[1090] - loss: 0.431765  acc: 83.0000%(107/128)\n",
      "Batch[1100] - loss: 0.496091  acc: 78.0000%(100/128)\n",
      "Batch[1110] - loss: 0.416179  acc: 85.0000%(110/128)\n",
      "Batch[1120] - loss: 0.298466  acc: 87.0000%(112/128)\n",
      "Batch[1130] - loss: 0.316742  acc: 89.0000%(115/128)\n",
      "Batch[1140] - loss: 0.284864  acc: 92.0000%(119/128)\n",
      "Batch[1150] - loss: 0.420848  acc: 84.0000%(108/128)\n",
      "Batch[1160] - loss: 0.328478  acc: 89.0000%(114/128)\n",
      "Batch[1170] - loss: 0.418801  acc: 82.0000%(106/128)\n",
      "\n",
      "Start epoch 4 ....\n",
      "Batch[1180] - loss: 0.413125  acc: 87.0000%(112/128)\n",
      "Batch[1190] - loss: 0.382684  acc: 84.0000%(108/128)\n",
      "Batch[1200] - loss: 0.392031  acc: 85.0000%(109/128)\n",
      "Batch[1210] - loss: 0.307811  acc: 88.0000%(113/128)\n",
      "Batch[1220] - loss: 0.319709  acc: 86.0000%(111/128)\n",
      "Batch[1230] - loss: 0.382995  acc: 83.0000%(107/128)\n",
      "Batch[1240] - loss: 0.279506  acc: 89.0000%(114/128)\n",
      "Batch[1250] - loss: 0.260723  acc: 92.0000%(118/128)\n",
      "Batch[1260] - loss: 0.341297  acc: 86.0000%(111/128)\n",
      "Batch[1270] - loss: 0.269854  acc: 90.0000%(116/128)\n",
      "Batch[1280] - loss: 0.266334  acc: 90.0000%(116/128)\n",
      "Batch[1290] - loss: 0.369695  acc: 88.0000%(113/128)\n",
      "Batch[1300] - loss: 0.221749  acc: 92.0000%(118/128)\n",
      "Batch[1310] - loss: 0.493441  acc: 82.0000%(106/128)\n",
      "Batch[1320] - loss: 0.296252  acc: 89.0000%(114/128)\n",
      "Batch[1330] - loss: 0.365821  acc: 84.0000%(108/128)\n",
      "Batch[1340] - loss: 0.273590  acc: 91.0000%(117/128)\n",
      "Batch[1350] - loss: 0.376545  acc: 88.0000%(113/128)\n",
      "Batch[1360] - loss: 0.411617  acc: 85.0000%(109/128)\n",
      "Batch[1370] - loss: 0.284494  acc: 90.0000%(116/128)\n",
      "Batch[1380] - loss: 0.315738  acc: 87.0000%(112/128)\n",
      "Batch[1390] - loss: 0.384510  acc: 87.0000%(112/128)\n",
      "Batch[1400] - loss: 0.507229  acc: 80.0000%(103/128)\n",
      "Batch[1410] - loss: 0.316933  acc: 87.0000%(112/128)\n",
      "Batch[1420] - loss: 0.271366  acc: 90.0000%(116/128)\n",
      "Batch[1430] - loss: 0.320766  acc: 89.0000%(114/128)\n",
      "Batch[1440] - loss: 0.327852  acc: 90.0000%(116/128)\n",
      "Batch[1450] - loss: 0.276222  acc: 89.0000%(114/128)\n",
      "Batch[1460] - loss: 0.341744  acc: 89.0000%(114/128)\n",
      "Batch[1470] - loss: 0.427714  acc: 86.0000%(111/128)\n",
      "Batch[1480] - loss: 0.376722  acc: 87.0000%(112/128)\n",
      "Batch[1490] - loss: 0.262018  acc: 92.0000%(118/128)\n",
      "Batch[1500] - loss: 0.309385  acc: 90.0000%(116/128)\n",
      "Batch[1510] - loss: 0.368275  acc: 83.0000%(107/128)\n",
      "Batch[1520] - loss: 0.332845  acc: 86.0000%(111/128)\n",
      "Batch[1530] - loss: 0.367956  acc: 88.0000%(113/128)\n",
      "Batch[1540] - loss: 0.284035  acc: 89.0000%(115/128)\n",
      "Batch[1550] - loss: 0.397935  acc: 85.0000%(109/128)\n",
      "Batch[1560] - loss: 0.240480  acc: 90.0000%(116/128)\n",
      "\n",
      "Start epoch 5 ....\n",
      "Batch[1570] - loss: 0.225810  acc: 92.0000%(119/128)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch[1580] - loss: 0.261080  acc: 89.0000%(114/128)\n",
      "Batch[1590] - loss: 0.276141  acc: 89.0000%(115/128)\n",
      "Batch[1600] - loss: 0.277655  acc: 91.0000%(117/128)\n",
      "Batch[1610] - loss: 0.350498  acc: 86.0000%(111/128)\n",
      "Batch[1620] - loss: 0.264771  acc: 91.0000%(117/128)\n",
      "Batch[1630] - loss: 0.230887  acc: 91.0000%(117/128)\n",
      "Batch[1640] - loss: 0.264432  acc: 91.0000%(117/128)\n",
      "Batch[1650] - loss: 0.342060  acc: 86.0000%(111/128)\n",
      "Batch[1660] - loss: 0.280505  acc: 90.0000%(116/128)\n",
      "Batch[1670] - loss: 0.256573  acc: 92.0000%(118/128)\n",
      "Batch[1680] - loss: 0.269033  acc: 89.0000%(115/128)\n",
      "Batch[1690] - loss: 0.231784  acc: 91.0000%(117/128)\n",
      "Batch[1700] - loss: 0.192448  acc: 95.0000%(122/128)\n",
      "Batch[1710] - loss: 0.271623  acc: 91.0000%(117/128)\n",
      "Batch[1720] - loss: 0.212034  acc: 91.0000%(117/128)\n",
      "Batch[1730] - loss: 0.254088  acc: 90.0000%(116/128)\n",
      "Batch[1740] - loss: 0.287040  acc: 89.0000%(115/128)\n",
      "Batch[1750] - loss: 0.218452  acc: 92.0000%(118/128)\n",
      "Batch[1760] - loss: 0.174430  acc: 93.0000%(120/128)\n",
      "Batch[1770] - loss: 0.241207  acc: 90.0000%(116/128)\n",
      "Batch[1780] - loss: 0.291353  acc: 86.0000%(111/128)\n",
      "Batch[1790] - loss: 0.204934  acc: 92.0000%(118/128)\n",
      "Batch[1800] - loss: 0.226998  acc: 92.0000%(118/128)\n",
      "Batch[1810] - loss: 0.228449  acc: 92.0000%(118/128)\n",
      "Batch[1820] - loss: 0.309031  acc: 89.0000%(114/128)\n",
      "Batch[1830] - loss: 0.336407  acc: 85.0000%(110/128)\n",
      "Batch[1840] - loss: 0.350787  acc: 85.0000%(110/128)\n",
      "Batch[1850] - loss: 0.319502  acc: 90.0000%(116/128)\n",
      "Batch[1860] - loss: 0.262538  acc: 90.0000%(116/128)\n",
      "Batch[1870] - loss: 0.303260  acc: 89.0000%(115/128)\n",
      "Batch[1880] - loss: 0.202000  acc: 91.0000%(117/128)\n",
      "Batch[1890] - loss: 0.312296  acc: 89.0000%(114/128)\n",
      "Batch[1900] - loss: 0.331472  acc: 89.0000%(114/128)\n",
      "Batch[1910] - loss: 0.303891  acc: 91.0000%(117/128)\n",
      "Batch[1920] - loss: 0.275223  acc: 92.0000%(118/128)\n",
      "Batch[1930] - loss: 0.362187  acc: 86.0000%(111/128)\n",
      "Batch[1940] - loss: 0.374191  acc: 87.0000%(112/128)\n",
      "Batch[1950] - loss: 0.325479  acc: 89.0000%(114/128)\n",
      "\n",
      "Start epoch 6 ....\n",
      "Batch[1960] - loss: 0.165010  acc: 93.0000%(120/128)\n",
      "Batch[1970] - loss: 0.226678  acc: 89.0000%(114/128)\n",
      "Batch[1980] - loss: 0.277499  acc: 89.0000%(114/128)\n",
      "Batch[1990] - loss: 0.228759  acc: 93.0000%(120/128)\n",
      "Batch[2000] - loss: 0.209692  acc: 91.0000%(117/128)\n",
      "Batch[2010] - loss: 0.221228  acc: 91.0000%(117/128)\n",
      "Batch[2020] - loss: 0.247565  acc: 92.0000%(118/128)\n",
      "Batch[2030] - loss: 0.269816  acc: 89.0000%(115/128)\n",
      "Batch[2040] - loss: 0.247163  acc: 90.0000%(116/128)\n",
      "Batch[2050] - loss: 0.199924  acc: 91.0000%(117/128)\n",
      "Batch[2060] - loss: 0.210852  acc: 92.0000%(118/128)\n",
      "Batch[2070] - loss: 0.264704  acc: 88.0000%(113/128)\n",
      "Batch[2080] - loss: 0.299302  acc: 90.0000%(116/128)\n",
      "Batch[2090] - loss: 0.213974  acc: 92.0000%(118/128)\n",
      "Batch[2100] - loss: 0.168358  acc: 93.0000%(120/128)\n",
      "Batch[2110] - loss: 0.193594  acc: 93.0000%(120/128)\n",
      "Batch[2120] - loss: 0.201494  acc: 91.0000%(117/128)\n",
      "Batch[2130] - loss: 0.225826  acc: 91.0000%(117/128)\n",
      "Batch[2140] - loss: 0.280163  acc: 87.0000%(112/128)\n",
      "Batch[2150] - loss: 0.226370  acc: 90.0000%(116/128)\n",
      "Batch[2160] - loss: 0.175839  acc: 92.0000%(118/128)\n",
      "Batch[2170] - loss: 0.274330  acc: 89.0000%(115/128)\n",
      "Batch[2180] - loss: 0.208579  acc: 91.0000%(117/128)\n",
      "Batch[2190] - loss: 0.109957  acc: 96.0000%(123/128)\n",
      "Batch[2200] - loss: 0.187381  acc: 92.0000%(119/128)\n",
      "Batch[2210] - loss: 0.177965  acc: 93.0000%(120/128)\n",
      "Batch[2220] - loss: 0.282511  acc: 92.0000%(118/128)\n",
      "Batch[2230] - loss: 0.179510  acc: 95.0000%(122/128)\n",
      "Batch[2240] - loss: 0.225077  acc: 92.0000%(119/128)\n",
      "Batch[2250] - loss: 0.283299  acc: 89.0000%(115/128)\n",
      "Batch[2260] - loss: 0.195225  acc: 93.0000%(120/128)\n",
      "Batch[2270] - loss: 0.199485  acc: 92.0000%(118/128)\n",
      "Batch[2280] - loss: 0.264262  acc: 95.0000%(122/128)\n",
      "Batch[2290] - loss: 0.279727  acc: 87.0000%(112/128)\n",
      "Batch[2300] - loss: 0.339399  acc: 89.0000%(114/128)\n",
      "Batch[2310] - loss: 0.246310  acc: 90.0000%(116/128)\n",
      "Batch[2320] - loss: 0.295601  acc: 89.0000%(115/128)\n",
      "Batch[2330] - loss: 0.175893  acc: 95.0000%(122/128)\n",
      "Batch[2340] - loss: 0.280437  acc: 90.0000%(116/128)\n",
      "\n",
      "Start epoch 7 ....\n",
      "Batch[2350] - loss: 0.299769  acc: 90.0000%(116/128)\n",
      "Batch[2360] - loss: 0.302036  acc: 92.0000%(118/128)\n",
      "Batch[2370] - loss: 0.212991  acc: 92.0000%(118/128)\n",
      "Batch[2380] - loss: 0.237627  acc: 91.0000%(117/128)\n",
      "Batch[2390] - loss: 0.215808  acc: 91.0000%(117/128)\n",
      "Batch[2400] - loss: 0.168895  acc: 94.0000%(121/128)\n",
      "Batch[2410] - loss: 0.136175  acc: 96.0000%(124/128)\n",
      "Batch[2420] - loss: 0.191720  acc: 93.0000%(120/128)\n",
      "Batch[2430] - loss: 0.220483  acc: 93.0000%(120/128)\n",
      "Batch[2440] - loss: 0.127605  acc: 96.0000%(123/128)\n",
      "Batch[2450] - loss: 0.206455  acc: 90.0000%(116/128)\n",
      "Batch[2460] - loss: 0.185219  acc: 93.0000%(120/128)\n",
      "Batch[2470] - loss: 0.199163  acc: 92.0000%(119/128)\n",
      "Batch[2480] - loss: 0.290250  acc: 91.0000%(117/128)\n",
      "Batch[2490] - loss: 0.125843  acc: 95.0000%(122/128)\n",
      "Batch[2500] - loss: 0.184254  acc: 93.0000%(120/128)\n",
      "Batch[2510] - loss: 0.202073  acc: 91.0000%(117/128)\n",
      "Batch[2520] - loss: 0.242678  acc: 91.0000%(117/128)\n",
      "Batch[2530] - loss: 0.224946  acc: 92.0000%(119/128)\n",
      "Batch[2540] - loss: 0.246633  acc: 92.0000%(118/128)\n",
      "Batch[2550] - loss: 0.225306  acc: 91.0000%(117/128)\n",
      "Batch[2560] - loss: 0.199678  acc: 92.0000%(119/128)\n",
      "Batch[2570] - loss: 0.280871  acc: 90.0000%(116/128)\n",
      "Batch[2580] - loss: 0.206296  acc: 92.0000%(119/128)\n",
      "Batch[2590] - loss: 0.213125  acc: 90.0000%(116/128)\n",
      "Batch[2600] - loss: 0.204894  acc: 92.0000%(118/128)\n",
      "Batch[2610] - loss: 0.284743  acc: 91.0000%(117/128)\n",
      "Batch[2620] - loss: 0.224847  acc: 94.0000%(121/128)\n",
      "Batch[2630] - loss: 0.202599  acc: 91.0000%(117/128)\n",
      "Batch[2640] - loss: 0.261775  acc: 89.0000%(115/128)\n",
      "Batch[2650] - loss: 0.126008  acc: 94.0000%(121/128)\n",
      "Batch[2660] - loss: 0.281194  acc: 87.0000%(112/128)\n",
      "Batch[2670] - loss: 0.130676  acc: 94.0000%(121/128)\n",
      "Batch[2680] - loss: 0.169317  acc: 93.0000%(120/128)\n",
      "Batch[2690] - loss: 0.232036  acc: 90.0000%(116/128)\n",
      "Batch[2700] - loss: 0.203229  acc: 89.0000%(115/128)\n",
      "Batch[2710] - loss: 0.152102  acc: 94.0000%(121/128)\n",
      "Batch[2720] - loss: 0.229340  acc: 92.0000%(118/128)\n",
      "Batch[2730] - loss: 0.191005  acc: 93.0000%(120/128)\n",
      "\n",
      "Start epoch 8 ....\n",
      "Batch[2740] - loss: 0.166918  acc: 92.0000%(119/128)\n",
      "Batch[2750] - loss: 0.172124  acc: 91.0000%(117/128)\n",
      "Batch[2760] - loss: 0.209783  acc: 93.0000%(120/128)\n",
      "Batch[2770] - loss: 0.134893  acc: 96.0000%(123/128)\n",
      "Batch[2780] - loss: 0.213358  acc: 91.0000%(117/128)\n",
      "Batch[2790] - loss: 0.191158  acc: 92.0000%(118/128)\n",
      "Batch[2800] - loss: 0.135724  acc: 95.0000%(122/128)\n",
      "Batch[2810] - loss: 0.186514  acc: 93.0000%(120/128)\n",
      "Batch[2820] - loss: 0.281398  acc: 89.0000%(115/128)\n",
      "Batch[2830] - loss: 0.245484  acc: 91.0000%(117/128)\n",
      "Batch[2840] - loss: 0.275434  acc: 92.0000%(118/128)\n",
      "Batch[2850] - loss: 0.265268  acc: 90.0000%(116/128)\n",
      "Batch[2860] - loss: 0.108230  acc: 96.0000%(124/128)\n",
      "Batch[2870] - loss: 0.202444  acc: 94.0000%(121/128)\n",
      "Batch[2880] - loss: 0.152935  acc: 94.0000%(121/128)\n",
      "Batch[2890] - loss: 0.229874  acc: 92.0000%(118/128)\n",
      "Batch[2900] - loss: 0.136110  acc: 95.0000%(122/128)\n",
      "Batch[2910] - loss: 0.155085  acc: 92.0000%(119/128)\n",
      "Batch[2920] - loss: 0.224525  acc: 91.0000%(117/128)\n",
      "Batch[2930] - loss: 0.231702  acc: 91.0000%(117/128)\n",
      "Batch[2940] - loss: 0.200330  acc: 92.0000%(119/128)\n",
      "Batch[2950] - loss: 0.293534  acc: 90.0000%(116/128)\n",
      "Batch[2960] - loss: 0.313812  acc: 89.0000%(115/128)\n",
      "Batch[2970] - loss: 0.077069  acc: 96.0000%(124/128)\n",
      "Batch[2980] - loss: 0.130887  acc: 94.0000%(121/128)\n",
      "Batch[2990] - loss: 0.160585  acc: 96.0000%(124/128)\n",
      "Batch[3000] - loss: 0.185917  acc: 96.0000%(124/128)\n",
      "Batch[3010] - loss: 0.136407  acc: 94.0000%(121/128)\n",
      "Batch[3020] - loss: 0.307442  acc: 90.0000%(116/128)\n",
      "Batch[3030] - loss: 0.184898  acc: 91.0000%(117/128)\n",
      "Batch[3040] - loss: 0.193598  acc: 93.0000%(120/128)\n",
      "Batch[3050] - loss: 0.153413  acc: 92.0000%(119/128)\n",
      "Batch[3060] - loss: 0.191501  acc: 93.0000%(120/128)\n",
      "Batch[3070] - loss: 0.149477  acc: 96.0000%(124/128)\n",
      "Batch[3080] - loss: 0.163685  acc: 93.0000%(120/128)\n",
      "Batch[3090] - loss: 0.180217  acc: 92.0000%(118/128)\n",
      "Batch[3100] - loss: 0.167503  acc: 92.0000%(119/128)\n",
      "Batch[3110] - loss: 0.154884  acc: 96.0000%(123/128)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch[3120] - loss: 0.250714  acc: 92.0000%(118/128)\n",
      "\n",
      "Start epoch 9 ....\n",
      "Batch[3130] - loss: 0.227372  acc: 89.0000%(114/128)\n",
      "Batch[3140] - loss: 0.177092  acc: 91.0000%(117/128)\n",
      "Batch[3150] - loss: 0.114697  acc: 95.0000%(122/128)\n",
      "Batch[3160] - loss: 0.141702  acc: 95.0000%(122/128)\n",
      "Batch[3170] - loss: 0.171061  acc: 92.0000%(118/128)\n",
      "Batch[3180] - loss: 0.109795  acc: 97.0000%(125/128)\n",
      "Batch[3190] - loss: 0.088957  acc: 99.0000%(127/128)\n",
      "Batch[3200] - loss: 0.132319  acc: 93.0000%(120/128)\n",
      "Batch[3210] - loss: 0.089116  acc: 98.0000%(126/128)\n",
      "Batch[3220] - loss: 0.109611  acc: 96.0000%(124/128)\n",
      "Batch[3230] - loss: 0.192636  acc: 92.0000%(118/128)\n",
      "Batch[3240] - loss: 0.305806  acc: 88.0000%(113/128)\n",
      "Batch[3250] - loss: 0.119055  acc: 96.0000%(124/128)\n",
      "Batch[3260] - loss: 0.127486  acc: 95.0000%(122/128)\n",
      "Batch[3270] - loss: 0.157948  acc: 93.0000%(120/128)\n",
      "Batch[3280] - loss: 0.220308  acc: 91.0000%(117/128)\n",
      "Batch[3290] - loss: 0.158623  acc: 93.0000%(120/128)\n",
      "Batch[3300] - loss: 0.115881  acc: 96.0000%(123/128)\n",
      "Batch[3310] - loss: 0.141154  acc: 94.0000%(121/128)\n",
      "Batch[3320] - loss: 0.119459  acc: 96.0000%(124/128)\n",
      "Batch[3330] - loss: 0.141455  acc: 94.0000%(121/128)\n",
      "Batch[3340] - loss: 0.295071  acc: 90.0000%(116/128)\n",
      "Batch[3350] - loss: 0.213704  acc: 93.0000%(120/128)\n",
      "Batch[3360] - loss: 0.247432  acc: 89.0000%(114/128)\n",
      "Batch[3370] - loss: 0.234402  acc: 93.0000%(120/128)\n",
      "Batch[3380] - loss: 0.184563  acc: 94.0000%(121/128)\n",
      "Batch[3390] - loss: 0.144835  acc: 94.0000%(121/128)\n",
      "Batch[3400] - loss: 0.119635  acc: 96.0000%(123/128)\n",
      "Batch[3410] - loss: 0.221825  acc: 92.0000%(118/128)\n",
      "Batch[3420] - loss: 0.270986  acc: 89.0000%(114/128)\n",
      "Batch[3430] - loss: 0.150827  acc: 92.0000%(118/128)\n",
      "Batch[3440] - loss: 0.132074  acc: 96.0000%(124/128)\n",
      "Batch[3450] - loss: 0.174633  acc: 92.0000%(118/128)\n",
      "Batch[3460] - loss: 0.144546  acc: 96.0000%(123/128)\n",
      "Batch[3470] - loss: 0.129544  acc: 95.0000%(122/128)\n",
      "Batch[3480] - loss: 0.172996  acc: 92.0000%(119/128)\n",
      "Batch[3490] - loss: 0.267753  acc: 89.0000%(114/128)\n",
      "Batch[3500] - loss: 0.190260  acc: 92.0000%(118/128)\n",
      "Batch[3510] - loss: 0.190808  acc: 91.0000%(117/128)\n",
      "\n",
      "Start epoch 10 ....\n",
      "Batch[3520] - loss: 0.174122  acc: 91.0000%(117/128)\n",
      "Batch[3530] - loss: 0.147802  acc: 95.0000%(122/128)\n",
      "Batch[3540] - loss: 0.125241  acc: 95.0000%(122/128)\n",
      "Batch[3550] - loss: 0.148842  acc: 92.0000%(118/128)\n",
      "Batch[3560] - loss: 0.100261  acc: 97.0000%(125/128)\n",
      "Batch[3570] - loss: 0.089121  acc: 97.0000%(125/128)\n",
      "Batch[3580] - loss: 0.185175  acc: 92.0000%(118/128)\n",
      "Batch[3590] - loss: 0.169371  acc: 93.0000%(120/128)\n",
      "Batch[3600] - loss: 0.107932  acc: 96.0000%(123/128)\n",
      "Batch[3610] - loss: 0.137895  acc: 92.0000%(118/128)\n",
      "Batch[3620] - loss: 0.128840  acc: 95.0000%(122/128)\n",
      "Batch[3630] - loss: 0.311020  acc: 87.0000%(112/128)\n",
      "Batch[3640] - loss: 0.164560  acc: 95.0000%(122/128)\n",
      "Batch[3650] - loss: 0.097635  acc: 98.0000%(126/128)\n",
      "Batch[3660] - loss: 0.153261  acc: 95.0000%(122/128)\n",
      "Batch[3670] - loss: 0.154522  acc: 96.0000%(124/128)\n",
      "Batch[3680] - loss: 0.100970  acc: 98.0000%(126/128)\n",
      "Batch[3690] - loss: 0.190366  acc: 93.0000%(120/128)\n",
      "Batch[3700] - loss: 0.202296  acc: 92.0000%(119/128)\n",
      "Batch[3710] - loss: 0.099892  acc: 95.0000%(122/128)\n",
      "Batch[3720] - loss: 0.168157  acc: 95.0000%(122/128)\n",
      "Batch[3730] - loss: 0.202184  acc: 95.0000%(122/128)\n",
      "Batch[3740] - loss: 0.132150  acc: 94.0000%(121/128)\n",
      "Batch[3750] - loss: 0.160022  acc: 94.0000%(121/128)\n",
      "Batch[3760] - loss: 0.112917  acc: 93.0000%(120/128)\n",
      "Batch[3770] - loss: 0.144246  acc: 95.0000%(122/128)\n",
      "Batch[3780] - loss: 0.159112  acc: 94.0000%(121/128)\n",
      "Batch[3790] - loss: 0.199846  acc: 92.0000%(118/128)\n",
      "Batch[3800] - loss: 0.138575  acc: 95.0000%(122/128)\n",
      "Batch[3810] - loss: 0.162831  acc: 94.0000%(121/128)\n",
      "Batch[3820] - loss: 0.140093  acc: 94.0000%(121/128)\n",
      "Batch[3830] - loss: 0.221607  acc: 92.0000%(119/128)\n",
      "Batch[3840] - loss: 0.064598  acc: 98.0000%(126/128)\n",
      "Batch[3850] - loss: 0.150686  acc: 92.0000%(119/128)\n",
      "Batch[3860] - loss: 0.136286  acc: 96.0000%(123/128)\n",
      "Batch[3870] - loss: 0.191108  acc: 94.0000%(121/128)\n",
      "Batch[3880] - loss: 0.205995  acc: 92.0000%(118/128)\n",
      "Batch[3890] - loss: 0.249337  acc: 91.0000%(117/128)\n",
      "Batch[3900] - loss: 0.121777  acc: 96.0000%(123/128)\n",
      "Batch[3910] - loss: 0.093779  acc: 96.0000%(50/52)\n",
      "Final training accuracy: 96\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'args' object has no attribute 'epoch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-96636c685165>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'-'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m89\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-30-96636c685165>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, test_loader, args)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0macc_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'{:.0f}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Final training accuracy: '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0macc_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'final_epoch_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_hl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'args' object has no attribute 'epoch'"
     ]
    }
   ],
   "source": [
    "def train(model, train_loader, test_loader, args):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "    model.train()\n",
    "    \n",
    "    steps = 0\n",
    "    best_acc = 0\n",
    "    \n",
    "    for epoch in range(1, args.epochs+1):\n",
    "        print('\\nStart epoch', epoch, '....')\n",
    "        for title, body, label in train_loader:\n",
    "            title_t, body_t, label_t = build_tensor(title, body, label)\n",
    "            optimizer.zero_grad()\n",
    "            logit = model(title_t, body_t)\n",
    "            loss = F.cross_entropy(logit, label_t)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            steps += 1\n",
    "            if steps % args.log_interval == 0:\n",
    "                corrects = (torch.max(logit, 1)[1].view(label_t.size()).data == label_t.data).sum()\n",
    "                accuracy = 100.0 * corrects/label_t.shape[0]\n",
    "                print(\n",
    "                    '\\rBatch[{}] - loss: {:.6f}  acc: {:.4f}%({}/{})'.format(steps,\n",
    "                                                                             loss.data[0], \n",
    "                                                                             accuracy,\n",
    "                                                                             corrects,\n",
    "                                                                             label_t.shape[0]))\n",
    "        \n",
    "            # Evaluate & save model after every interval\n",
    "#             if steps % args.save_interval == 0 and steps > 1500:\n",
    "#                 corrects = (torch.max(logit, 1)[1].view(label_t.size()).data == label_t.data).sum()\n",
    "#                 accuracy = 100.0 * corrects/label_t.shape[0]\n",
    "#                 print(\n",
    "#                     '\\rBatch[{}] - loss: {:.6f}  acc: {:.4f}%({}/{})'.format(steps,\n",
    "#                                                                              loss.data[0], \n",
    "#                                                                              accuracy,\n",
    "#                                                                              corrects,\n",
    "#                                                                              label_t.shape[0]))\n",
    "#                 print('Saving snapshot result ...')\n",
    "#                 acc_str = '{:.0f}'.format(accuracy)\n",
    "#                 save(model, args.save_dir, 'snapshot_acc_' + acc_str, steps)\n",
    "                \n",
    "    # save final model\n",
    "    corrects = (torch.max(logit, 1)[1].view(label_t.size()).data == label_t.data).sum()\n",
    "    accuracy = 100.0 * corrects/label_t.shape[0]\n",
    "    acc_str = '{:.0f}'.format(accuracy)\n",
    "    print('Final training accuracy: ' + acc_str)\n",
    "    save(model, args.save_dir, 'final_epoch_' + str(args.epochs) + '_hl', 3)\n",
    "\n",
    "\n",
    "try:\n",
    "    train(model, train_loader, test_loader, args)\n",
    "except KeyboardInterrupt:\n",
    "    print('\\n' + '-' * 89)\n",
    "    print('Exiting from training early')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_test(test_loader, model, args):\n",
    "    pred = []\n",
    "    gold = []\n",
    "    \n",
    "    # restore the best parameters\n",
    "#     model_file = os.path.join(args.save_dir, 'final_acc_84_steps_40000.pt')\n",
    "#     model.load_state_dict(torch.load(model_file, map_location=lambda storage, loc: storage))\n",
    "    \n",
    "    model.eval()\n",
    "    corrects, avg_loss, total = 0, 0, 0\n",
    "    for title, body, label in test_loader:\n",
    "        title_t, body_t, label_t = build_tensor(title, body, label)\n",
    "        logit = model(title_t, body_t)\n",
    "        loss = F.cross_entropy(logit, label_t, size_average=False)\n",
    "        avg_loss += loss.data[0]\n",
    "        corrects += (torch.max(logit, 1)[1].view(label_t.size()).data == label_t.data).sum()\n",
    "        total += len(title)\n",
    "        tmp = torch.max(logit, 1)[1]\n",
    "        pred = pred + tmp.cpu().numpy().tolist()\n",
    "        gold = gold + label_t.cpu().numpy().tolist()\n",
    "\n",
    "    size = len(test_loader.dataset)\n",
    "    avg_loss /= size\n",
    "    accuracy = 100.0 * corrects/size\n",
    "    print('\\nEvaluation - loss: {:.6f}  acc: {:.4f}%({}/{}) \\n'.format(avg_loss, \n",
    "                                                                       accuracy, \n",
    "                                                                       corrects, \n",
    "                                                                       size))\n",
    "    return pred, gold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rifkiaputri/anaconda2/envs/nlp_project/lib/python3.6/site-packages/ipykernel_launcher.py:15: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation - loss: 1.600614  acc: 64.0000%(16325/25413) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred, gold = predict_test(test_loader, model, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils.score as sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------\n",
      "|           |   agree   | disagree  |  discuss  | unrelated |\n",
      "-------------------------------------------------------------\n",
      "|   agree   |    794    |     3     |    400    |    706    |\n",
      "-------------------------------------------------------------\n",
      "| disagree  |    304    |     2     |    133    |    258    |\n",
      "-------------------------------------------------------------\n",
      "|  discuss  |   1042    |    20     |   2167    |   1235    |\n",
      "-------------------------------------------------------------\n",
      "| unrelated |   2411    |     5     |   2571    |   13362   |\n",
      "-------------------------------------------------------------\n",
      "Score: 6779.0 out of 11651.25\t(58.18259843364446%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "58.18259843364446"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.report_score([sc.LABELS[e] for e in gold], [sc.LABELS[e] for e in pred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
