{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = [\n",
    "        \"a\", \"about\", \"above\", \"across\", \"after\", \"afterwards\", \"again\", \"against\", \"all\", \"almost\", \"alone\", \"along\",\n",
    "        \"already\", \"also\", \"although\", \"always\", \"am\", \"among\", \"amongst\", \"amoungst\", \"amount\", \"an\", \"and\", \"another\",\n",
    "        \"any\", \"anyhow\", \"anyone\", \"anything\", \"anyway\", \"anywhere\", \"are\", \"around\", \"as\", \"at\", \"back\", \"be\",\n",
    "        \"became\", \"because\", \"become\", \"becomes\", \"becoming\", \"been\", \"before\", \"beforehand\", \"behind\", \"being\",\n",
    "        \"below\", \"beside\", \"besides\", \"between\", \"beyond\", \"bill\", \"both\", \"bottom\", \"but\", \"by\", \"call\", \"can\", \"co\",\n",
    "        \"con\", \"could\", \"cry\", \"de\", \"describe\", \"detail\", \"do\", \"done\", \"down\", \"due\", \"during\", \"each\", \"eg\", \"eight\",\n",
    "        \"either\", \"eleven\", \"else\", \"elsewhere\", \"empty\", \"enough\", \"etc\", \"even\", \"ever\", \"every\", \"everyone\",\n",
    "        \"everything\", \"everywhere\", \"except\", \"few\", \"fifteen\", \"fifty\", \"fill\", \"find\", \"fire\", \"first\", \"five\", \"for\",\n",
    "        \"former\", \"formerly\", \"forty\", \"found\", \"four\", \"from\", \"front\", \"full\", \"further\", \"get\", \"give\", \"go\", \"had\",\n",
    "        \"has\", \"have\", \"he\", \"hence\", \"her\", \"here\", \"hereafter\", \"hereby\", \"herein\", \"hereupon\", \"hers\", \"herself\",\n",
    "        \"him\", \"himself\", \"his\", \"how\", \"however\", \"hundred\", \"i\", \"ie\", \"if\", \"in\", \"inc\", \"indeed\", \"interest\",\n",
    "        \"into\", \"is\", \"it\", \"its\", \"itself\", \"keep\", \"last\", \"latter\", \"latterly\", \"least\", \"less\", \"ltd\", \"made\",\n",
    "        \"many\", \"may\", \"me\", \"meanwhile\", \"might\", \"mill\", \"mine\", \"more\", \"moreover\", \"most\", \"mostly\", \"move\", \"much\",\n",
    "        \"must\", \"my\", \"myself\", \"name\", \"namely\", \"neither\", \"nevertheless\", \"next\", \"nine\", \"nobody\", \"now\", \"nowhere\",\n",
    "        \"of\", \"off\", \"often\", \"on\", \"once\", \"one\", \"only\", \"onto\", \"or\", \"other\", \"others\", \"otherwise\", \"our\", \"ours\",\n",
    "        \"ourselves\", \"out\", \"over\", \"own\", \"part\", \"per\", \"perhaps\", \"please\", \"put\", \"rather\", \"re\", \"same\", \"see\",\n",
    "        \"serious\", \"several\", \"she\", \"should\", \"show\", \"side\", \"since\", \"sincere\", \"six\", \"sixty\", \"so\", \"some\",\n",
    "        \"somehow\", \"someone\", \"something\", \"sometime\", \"sometimes\", \"somewhere\", \"still\", \"such\", \"system\", \"take\",\n",
    "        \"ten\", \"than\", \"that\", \"the\", \"their\", \"them\", \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\",\n",
    "        \"therefore\", \"therein\", \"thereupon\", \"these\", \"they\", \"thick\", \"thin\", \"third\", \"this\", \"those\", \"though\",\n",
    "        \"three\", \"through\", \"throughout\", \"thru\", \"thus\", \"to\", \"together\", \"too\", \"top\", \"toward\", \"towards\", \"twelve\",\n",
    "        \"twenty\", \"two\", \"un\", \"under\", \"until\", \"up\", \"upon\", \"us\", \"very\", \"via\", \"was\", \"we\", \"well\", \"were\", \"what\",\n",
    "        \"whatever\", \"when\", \"whence\", \"whenever\", \"where\", \"whereafter\", \"whereas\", \"whereby\", \"wherein\", \"whereupon\",\n",
    "        \"wherever\", \"whether\", \"which\", \"while\", \"whither\", \"who\", \"whoever\", \"whole\", \"whom\", \"whose\", \"why\", \"will\",\n",
    "        \"with\", \"within\", \"without\", \"would\", \"yet\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\"\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import csv\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "\n",
    "class Mydataset(Dataset):\n",
    "    def __init__(self, train=True):\n",
    "        def clean_str(string):\n",
    "            string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "            string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "            string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "            string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "            string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "            string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "            string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "            string = re.sub(r\",\", \" , \", string)\n",
    "            string = re.sub(r\"!\", \" ! \", string)\n",
    "            string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "            string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "            string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "            string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "            return string.strip().lower()\n",
    "        if train:\n",
    "            print('loading trainig dataset')\n",
    "            bf = 'train_bodies.csv'\n",
    "            sf = 'train_stances.csv'\n",
    "        else:\n",
    "            print('loading testing dataset')\n",
    "            bf = 'competition_test_bodies.csv'\n",
    "            sf = 'competition_test_stances.csv'\n",
    "            \n",
    "        with open(os.path.join('fnc-1', sf), 'r', newline='', encoding='utf-8') as myFile:  \n",
    "            rdr = csv.reader(myFile)\n",
    "            next(rdr)\n",
    "            temp = list(rdr)\n",
    "            self.stances = [[clean_str(a[0]), a[1], a[2]] for a in temp]\n",
    "            print(len(self.stances), 'stances')\n",
    "\n",
    "        with open(os.path.join('fnc-1', bf), 'r', newline='', encoding='utf-8') as myFile:  \n",
    "            rdr = csv.reader(myFile)\n",
    "            next(rdr)\n",
    "            temp = list(rdr)\n",
    "            self.bodies = dict([[a[0], clean_str(a[1])]for a in temp])\n",
    "            print(len(self.bodies), 'bodies')\n",
    "            \n",
    "        self.temp = [a[0] for a in self.stances] + list(self.bodies.values())\n",
    "\n",
    "        self.len = len(self.stances)\n",
    "        self.labels = list(sorted(set([t[2] for t in self.stances])))\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.stances[index][0], self.bodies[self.stances[index][1]], self.stances[index][2]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "        \n",
    "    def get_labels(self):\n",
    "        return self.labels\n",
    "    \n",
    "    def get_label(self, id):\n",
    "        return self.labels[id]\n",
    "    \n",
    "    def get_label_id(self, label):\n",
    "        return self.labels.index(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading trainig dataset\n",
      "49972 stances\n",
      "1683 bodies\n",
      "loading testing dataset\n",
      "25413 stances\n",
      "904 bodies\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 128\n",
    "\n",
    "train_dataset = Mydataset()\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "test_dataset = Mydataset(train=False)\n",
    "test_loader = DataLoader(dataset=test_dataset,\n",
    "                         batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "N_LABELS = len(train_dataset.get_labels())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = list(test_dataset.bodies.values())\n",
    "a = [len(b) for b in a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19661 28 2219.6172566371683\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "400"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(max(a), min(a), sum(a)/len(a))\n",
    "a.index(28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [len(a[0]) for a in train_dataset.stances]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "226 9 69.37060754022252\n"
     ]
    }
   ],
   "source": [
    "print(max(a), min(a), sum(a)/len(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set()\n",
    "for stances in train_dataset.temp:\n",
    "    vocab.update(stances.split(' '))\n",
    "vocab = vocab - set(stop_words)\n",
    "vocab = list(sorted(vocab))\n",
    "\n",
    "def get_index(word):\n",
    "    if word in vocab:\n",
    "        return vocab.index(word) + 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\settings\\anaconda\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import word2vec.wordvector as w2v\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "class CNN_Text(nn.Module):\n",
    "    \n",
    "    def __init__(self, args):\n",
    "        super(CNN_Text, self).__init__()\n",
    "        self.args = args\n",
    "        \n",
    "        C = args.class_num\n",
    "        Ci = 1\n",
    "        Co = args.kernel_num\n",
    "        Ks = args.kernel_sizes\n",
    "        V = args.embed_num\n",
    "        D = args.embed_dim\n",
    "        \n",
    "        self.embed = nn.Embedding(V, D)\n",
    "        \n",
    "#         self.embed, V, D = w2v.get_embedding()\n",
    "\n",
    "        \n",
    "        self.embed = nn.Embedding(V, D)\n",
    "        \n",
    "        self.convs1 = nn.ModuleList([nn.Conv2d(Ci, Co, (K, D)) for K in Ks])\n",
    "        \n",
    "        self.dropout = nn.Dropout(args.dropout)\n",
    "        \n",
    "        self.fc1 = nn.Linear(len(Ks)*Co, C)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)  # (N, W, D)\n",
    "        \n",
    "        if self.args.static:\n",
    "            x = Variable(x)\n",
    "\n",
    "        x = x.unsqueeze(1)  # (N, Ci, W, D)\n",
    "\n",
    "        x = [F.relu(conv(x)).squeeze(3) for conv in self.convs1]  # [(N, Co, W), ...]*len(Ks)\n",
    "\n",
    "        x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x]  # [(N, Co), ...]*len(Ks)\n",
    "\n",
    "        x = torch.cat(x, 1)\n",
    "\n",
    "        x = self.dropout(x)  # (N, len(Ks)*Co)\n",
    "\n",
    "        logit = self.fc1(x)  # (N, C)\n",
    "        \n",
    "        return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class args:\n",
    "    pass\n",
    "\n",
    "args = args()\n",
    "\n",
    "args.class_num = 4\n",
    "args.kernel_num = 2\n",
    "args.kernel_sizes = [2,3,4,5,6]\n",
    "args.dropout = 0.5\n",
    "args.static = True\n",
    "args.embed_num = len(vocab) + 1\n",
    "args.embed_dim = 50\n",
    "args.lr = 0.001\n",
    "args.epochs = 256\n",
    "model = CNN_Text(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tensor(titles, bodys, labels):\n",
    "    label_t = torch.tensor([train_dataset.get_label_id(l) for l in labels], dtype=torch.long)\n",
    "    \n",
    "    title_i = [[get_index(w) for w in title.split(' ') if w not in stop_words] for title in titles]\n",
    "    title_size = max([len(a) for a in title_i])\n",
    "    title_t = torch.zeros(len(title_i), title_size, dtype=torch.long)\n",
    "    for li, title in enumerate(title_i):\n",
    "        for co, word in enumerate(title):\n",
    "            title_t[li][co] = word\n",
    "\n",
    "    body_i = [[get_index(w) for w in body.split(' ') if w not in stop_words] for body in bodys]\n",
    "    body_size = max([len(a) for a in body_i])\n",
    "    body_t = torch.zeros(len(body_i), body_size, dtype=torch.long)\n",
    "    for li, body in enumerate(body_i):\n",
    "        for co, word in enumerate(body):\n",
    "            body_t[li][co] = word\n",
    "    return title_t, body_t, label_t "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\settings\\anaconda\\lib\\site-packages\\ipykernel\\__main__.py:28: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch[1] - loss: 2.384413  acc: 5.0000%(7/128)\n",
      "Batch[2] - loss: 2.143292  acc: 7.0000%(9/128)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\settings\\anaconda\\lib\\site-packages\\ipykernel\\__main__.py:42: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "args.log_interval = 1\n",
    "args.test_interval = 2\n",
    "\n",
    "def train(model, train_loader, test_loader, args):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "    model.train()\n",
    "    \n",
    "    steps = 0\n",
    "    \n",
    "    for epoch in range(1, args.epochs+1):\n",
    "        batch = 0\n",
    "        for title, body, label in train_loader:\n",
    "            title_t, body_t, label_t = build_tensor(title, body, label)\n",
    "            optimizer.zero_grad()\n",
    "            logit = model(torch.cat([title_t, body_t], 1))\n",
    "            loss = F.cross_entropy(logit, label_t)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            steps += 1\n",
    "            if steps % args.log_interval == 0:\n",
    "                corrects = (torch.max(logit, 1)[1].view(label_t.size()).data == label_t.data).sum()\n",
    "                accuracy = 100.0 * corrects/label_t.shape[0]\n",
    "                print(\n",
    "                    '\\rBatch[{}] - loss: {:.6f}  acc: {:.4f}%({}/{})'.format(steps,\n",
    "                                                                             loss.data[0], \n",
    "                                                                             accuracy,\n",
    "                                                                             corrects,\n",
    "                                                                             label_t.shape[0]))\n",
    "            if steps % args.test_interval == 0:\n",
    "                acc = eval(test_loader, model, args)\n",
    "                \n",
    "def eval(test_loader, model, args):\n",
    "    model.eval()\n",
    "    corrects, avg_loss = 0, 0\n",
    "    for title, body, label in train_loader:\n",
    "        title_t, body_t, label_t = build_tensor(title, body, label)\n",
    "        logit = model(torch.cat([title_t, body_t], 1))\n",
    "        loss = F.cross_entropy(logit, label_t, size_average=False)\n",
    "        avg_loss += loss.data[0]\n",
    "        corrects += (torch.max(logit, 1)[1].view(label_t.size()).data == label_t.data).sum()\n",
    "    size = len(test_loader.dataset)\n",
    "    avg_loss /= size\n",
    "    accuracy = 100.0 * corrects/size\n",
    "    print('\\nEvaluation - loss: {:.6f}  acc: {:.4f}%({}/{}) \\n'.format(avg_loss, \n",
    "                                                                       accuracy, \n",
    "                                                                       corrects, \n",
    "                                                                       size))\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "try:\n",
    "    train(model, train_loader, test_loader, args)\n",
    "except KeyboardInterrupt:\n",
    "    print('\\n' + '-' * 89)\n",
    "    print('Exiting from training early')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
